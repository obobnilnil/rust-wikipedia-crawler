![image](https://github.com/user-attachments/assets/08a14fd6-d630-477c-8c45-3a6590f7ef07)
ğŸ•·ï¸ rust_crawler
A simple asynchronous web crawler written in Rust, designed for learning and experimenting with crawling logic on websites like Wikipedia.

âœ… By default, the example is configured to crawl Wikipedia (https://en.wikipedia.org)<br>
âœ… But it can be adapted to other websites by changing configuration and tweaking the link extraction logic

## Dependencies<br>
reqwest â€” HTTP client with async support (gzip, brotli, deflate, rustls-tls)

scraper â€” HTML parsing and CSS selector engine

tokio â€” Asynchronous runtime

## âš™ï¸ How it works
âœ… Reads configuration from CrawlerConfig struct, including:

base_url: base site (e.g., https://en.wikipedia.org)

seed_path: initial path to crawl from (e.g., /wiki/Main_Page)

respect_robots: whether to honor robots.txt (âš ï¸ should be true in real usage!)

max_pages: maximum pages to visit

max_depth: how deep to follow links from the seed

delay_ms: delay between requests (milliseconds)

âœ… Fetches and parses robots.txt<br>
âœ… Extracts links under /wiki/<br>
âœ… Skips links like /wiki/Special:, /wiki/Help: etc.<br>
âœ… Uses VecDeque as a queue and HashSet to avoid revisiting pages<br>

## ğŸ’¡ Can it crawl any website?
The current implementation is set up and tested on Wikipedia, using:

if href.starts_with("/wiki/") && !href.contains(':')
This filters links like:

âœ… /wiki/Some_Page â†’ crawl

âŒ /wiki/Special:Page â†’ skip

If you want to use it on other websites, you need to:

Change base_url and seed_path in the config

Adjust the href filter logic in the code to match that siteâ€™s URL structure

## ğŸš€ How to run
# Clone the project
```bash
git clone https://github.com/obobnilnil/rust-wikipedia-crawler.git
cd rust_crawler

# Build and run
cargo run
```
## ğŸ–¥ï¸ Example crawl result
Below is an example output when running the crawler on Wikipedia:
```bash
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Main_Page
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Wikipedia
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Free_content
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Encyclopedia
ğŸŒ Visiting: https://en.wikipedia.org/wiki/English_language
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Goblin_shark
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Shark
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Living_fossil
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Family_(biology)
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Mitsukurinidae
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Ampulla_of_Lorenzini
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Electric_field
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Continental_margin
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Submarine_canyon
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Seamount
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Teleost
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Cephalopod
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Crustacean
ğŸŒ Visiting: https://en.wikipedia.org/wiki/Water_column
ğŸŒ Visiting: https://en.wikipedia.org/wiki/International_Union_for_Conservation_of_Nature
ğŸ›‘ Reached max pages: 20
âœ… Crawl completed. Pages visited: 20
```
<p align="center">
  <img src="https://github.com/user-attachments/assets/2a6eedc8-10c2-48fe-8994-01fb1d4099ef" alt="robots.txt exmaple" />
</p>

And here is a screenshot of the corresponding Wikipedia page:
<p align="center">
  <img src="https://github.com/user-attachments/assets/5574e671-6fbf-461f-a2a9-f293e8347917" alt="robots.txt exmaple" />
</p>
This helps verify that the extracted links match real pages on the site.


## âš ï¸ Important notes
âš ï¸ Respect robots.txt!

This project has a config flag:
```bash
respect_robots: false
**Note:** The default is set to `false` for testing and learning purposes.
```

ğŸ‘‰ **In real-world use, you should set this to `true` to comply with site rules and respect `robots.txt`.**

## âŒ Ignoring `robots.txt` or crawling without permission can result in:

- IP bans
- Legal notices (for aggressive scraping)
- Breaking terms of service

This code is intended **for educational purposes only.**

## ğŸ” How to check robots.txt of any website
Most websites define their crawling rules in a robots.txt file located at:
```bash
https://<website-domain>/robots.txt
```
âœ… Example for Wikipedia, Shopee:
```bash
https://en.wikipedia.org/robots.txt
https://shopee.co.th/robots.txt
```
<p align="center">
  <img src="https://github.com/user-attachments/assets/6ee813c2-ac84-4016-8f4c-2a77bfe969c6" alt="robots.txt example" />
</p>
This file tells crawlers:

Which paths are allowed or disallowed

Crawl-delay or other special rules (if any)

Before running a crawler, you should always:

1ï¸âƒ£ Visit the websiteâ€™s robots.txt manually in a browser or using curl

2ï¸âƒ£ Look for lines like:
```bash
User-agent: *
Disallow: /private/
Disallow: /admin/
```
3ï¸âƒ£ Make sure your crawler respects these rules
By setting `respect_robots: true` in the config, the crawler will skip disallowed paths automatically.

## âœ¨ License: MIT License Â© 2025 Bank Worapon



